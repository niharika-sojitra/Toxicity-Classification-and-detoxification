{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4QGAt01tpXrf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('C:/Users/JOY/Desktop/Final Project')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import tf.keras.preprocessing.text as tk\n",
    "import tf.keras.preprocessing.sequence as ps\n",
    "from tensorflow.keras.layers import Input, Dropout, Bidirectional, Dense, Embedding, Conv1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "import Caribe as cb\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XA8asAkbnth6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39130     \"\\n\\n happy birthday! \\n\\nhey,  .  just stoppi...\n",
      "25049     i have no problem with the homosexual organisa...\n",
      "437       i never fucking made this mother fucking artic...\n",
      "93449     sonic the hedgehog \\n\\nsonic the hedgehog (ソニッ...\n",
      "35166     later, due to a corruption of the meaning, bot...\n",
      "                                ...                        \n",
      "73349     who is in charge of russia? putin or medevev? ...\n",
      "109259                        on edit: forgot to actually .\n",
      "50057                     \"\\ncopying to talk page. serenc \"\n",
      "5192                re-adding anti-barnstar\\n anti-barnstar\n",
      "128037    fuck wikipedia!!! \\n\\npeace (except to wikipedia)\n",
      "Name: comment_text, Length: 111699, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('datasets/jigsaw-toxic-comment-classification-challenge','train.csv', 'train.csv'))\n",
    "GLOVE_EMBEDDING = \"datasets/jigsaw-toxic-comment-classification-challenge/glove.6B.100d.txt\"\n",
    "\n",
    "column_names=[\"short\",\"long\"]\n",
    "df1 = pd.read_csv(\"datasets/abbrevations.csv\",names=column_names)\n",
    "df2 = pd.read_csv(\"datasets/emoji_df.csv\")\n",
    "df3 = pd.read_csv(\"datasets/twitterSlang.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.3, random_state = 1)\n",
    "\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"comment_text\"].fillna(\"fillna\")\n",
    "test[\"comment_text\"].fillna(\"fillna\")\n",
    "\n",
    "x_train = train[\"comment_text\"].str.lower()\n",
    "x_test = test[\"comment_text\"].str.lower()\n",
    "\n",
    "y_train = train[[\"Toxic\", \"Severe Toxic\", \"Obscene\", \"Threat\", \"Insult\", \"Identity Hate\"]].values\n",
    "y_test = test[[\"Toxic\", \"Severe Toxic\", \"Obscene\", \"Threat\", \"Insult\", \"Identity Hate\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 100000\n",
    "max_len = 150\n",
    "\n",
    "embed_size = 100\n",
    "\n",
    "tokenizer = tk.Tokenizer(num_words=max_words, lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train = ps.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = ps.pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JwR3B5Axpokk"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(GLOVE_EMBEDDING, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        embed = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embed\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embed_size), dtype='float32')\n",
    "\n",
    "for word, i in word_index.items():\n",
    "\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ChchJafsqLVQ"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "\n",
    "x = Embedding(max_words, embed_size, weights=[embedding_matrix], trainable=False)(input)\n",
    "\n",
    "x = Bidirectional(tf.keras.layers.GRU(128, return_sequences=True, dropout=0.1,\n",
    "                                                      recurrent_dropout=0.1))(x)\n",
    "\n",
    "x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "\n",
    "x = concatenate([avg_pool, max_pool])\n",
    "\n",
    "preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(input, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geXcXukbrdhN",
    "outputId": "5101e504-3ace-4550-b1e7-b4e8653f8d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 150, 100)     10000000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 150, 256)    176640      ['embedding_1[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 148, 64)      49216       ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 64)          0           ['conv1d_1[0][0]']               \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 64)          0           ['conv1d_1[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 128)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6)            774         ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,226,630\n",
      "Trainable params: 226,630\n",
      "Non-trainable params: 10,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Program Files\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=1e-3), metrics=['acc',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUXKkxL4rhTc",
    "outputId": "109b8f01-6e6b-49b9-f8a2-38a0ef6f54ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "699/699 [==============================] - ETA: 0s - loss: 0.0694 - acc: 0.9110 - f1_m: 0.5944 - precision_m: 0.7433 - recall_m: 0.5188\n",
      "Epoch 00001: saving model to training_1\\cp.ckpt\n",
      "699/699 [==============================] - 812s 1s/step - loss: 0.0694 - acc: 0.9110 - f1_m: 0.5944 - precision_m: 0.7433 - recall_m: 0.5188 - val_loss: 0.0584 - val_acc: 0.9770 - val_f1_m: 0.7029 - val_precision_m: 0.7111 - val_recall_m: 0.7053\n",
      "Epoch 2/3\n",
      "699/699 [==============================] - ETA: 0s - loss: 0.0516 - acc: 0.9330 - f1_m: 0.6988 - precision_m: 0.8097 - recall_m: 0.6294\n",
      "Epoch 00002: saving model to training_1\\cp.ckpt\n",
      "699/699 [==============================] - 805s 1s/step - loss: 0.0516 - acc: 0.9330 - f1_m: 0.6988 - precision_m: 0.8097 - recall_m: 0.6294 - val_loss: 0.0495 - val_acc: 0.9822 - val_f1_m: 0.7075 - val_precision_m: 0.7984 - val_recall_m: 0.6448\n",
      "Epoch 3/3\n",
      "699/699 [==============================] - ETA: 0s - loss: 0.0465 - acc: 0.9387 - f1_m: 0.7280 - precision_m: 0.8261 - recall_m: 0.6648\n",
      "Epoch 00003: saving model to training_1\\cp.ckpt\n",
      "699/699 [==============================] - 807s 1s/step - loss: 0.0465 - acc: 0.9387 - f1_m: 0.7280 - precision_m: 0.8261 - recall_m: 0.6648 - val_loss: 0.0483 - val_acc: 0.8464 - val_f1_m: 0.7157 - val_precision_m: 0.7878 - val_recall_m: 0.6662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x201a3646608>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,save_weights_only=True,verbose=1)\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),tf.keras.callbacks.TensorBoard(log_dir='./logs'),cp_callback]\n",
    "\n",
    "model.fit(x_train, y_train, validation_split=0.2, batch_size=batch_size, epochs=3, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Latest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhCsZDigrmVY",
    "outputId": "3cf939fd-361b-40e6-e352-9865a197cf52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  23 997 129  47   6\n",
      " 121   2 438  30  23  94]\n",
      "['article creation where would i go to start an article thanks']\n",
      "[0 0 0 0 0 0]\n",
      "[[6.3482631e-04 5.3460582e-05 2.8034404e-04 4.6893547e-05 5.3904607e-04\n",
      "  4.4846576e-05]]\n"
     ]
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.expand_dims(x_train[43], 0))\n",
    "\n",
    "print(tokenizer.sequences_to_texts([x_train[43]]))\n",
    "print(y_train[43])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496/1496 [==============================] - 226s 151ms/step - loss: 0.0494 - acc: 0.8488 - f1_m: 0.6401 - precision_m: 0.7228 - recall_m: 0.6144\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7227891683578491, Recall:0.6144306063652039, Accuracy:0.8487842679023743, F1:0.6400859355926514\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {precision}, Recall:{recall}, Accuracy:{accuracy}, F1:{f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "LgTHKTperp4b"
   },
   "outputs": [],
   "source": [
    "model.save('models/rnn03.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown metric function: f1_m. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2816/2702586816.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/rnn03.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"f1_score\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf1_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"precision\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprecision_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"recall\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrecall_m\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mJ:\\Program Files\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Program Files\\Anaconda3\\envs\\python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    707\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         raise ValueError(\n\u001b[1;32m--> 709\u001b[1;33m             \u001b[1;34mf'Unknown {printable_module_name}: {object_name}. Please ensure '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m             \u001b[1;34m'this object is passed to the `custom_objects` argument. See '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[1;34m'https://www.tensorflow.org/guide/keras/save_and_serialize'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown metric function: f1_m. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/rnn03.h5', custom_objects={\"f1_score\": f1_m, \"precision\": precision_m, \"recall\": recall_m})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingfilters(comment):\n",
    "    print(\"\\nOriginal sentence : \" ,comment)\n",
    "\n",
    "    #Slang \n",
    "    for i in df3.loc[:, 'slang']:\n",
    "        comment = comment.lower()\n",
    "        if(i.lower() in comment.split(\" \")):\n",
    "            j = df3[i==df3['slang']]\n",
    "            k = j.iloc[-1].values\n",
    "\n",
    "        comment = comment.lower()\n",
    "        comment = comment.replace(i.lower(), k[1])\n",
    "        print(\"Slang Expanded sentence : \" ,comment)\n",
    "\n",
    "    # Abberivation\n",
    "    for i in df1.loc[:, 'short']:\n",
    "        comment = comment.lower()\n",
    "        if(i.lower() in comment.split(\" \")):\n",
    "            j = df1[i==df1['short']]\n",
    "            k = j.iloc[-1].values\n",
    "\n",
    "        comment = comment.lower()\n",
    "        comment = comment.replace(i.lower(), k[1])\n",
    "        print(\"Abberivation Expanded sentence : \" ,comment)\n",
    "\n",
    "    # Emojis\n",
    "    for i in df2.loc[:, 'emoji']:\n",
    "        if(i in comment):\n",
    "            j = df2[i==df2['emoji']]\n",
    "            k = j.iloc[-1].values\n",
    "\n",
    "        comment = comment.replace(i, \", having \" +k[1])\n",
    "        print(\"Emojis Expanded sentence : \" ,comment)\n",
    "\n",
    "    comment=cb.caribe_corrector(comment)\n",
    "    print(\"Correct sentence : \" ,comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing detoxification function from detox file\n",
    "from detox import detoxification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI Action Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_comment(comment):\n",
    "    fcomment = preprocessingfilters(comment)\n",
    "    comment1 = tokenizer.texts_to_sequences([comment])\n",
    "    comment1 = tf.keras.preprocessing.sequence.pad_sequences(comment1, maxlen=max_len)\n",
    "    \n",
    "    results = model.predict(np.expand_dims(comment1[0], 0))\n",
    "    \n",
    "    if results[0][0] > 0.1:\n",
    "        pred = detoxification(comment)\n",
    "        pred = (str(pred)[1:-1]).strip('[\\]\",')\n",
    "        pred = str(pred)\n",
    "    else:\n",
    "        pred = \"Non Toxic Sentence\" \n",
    "\n",
    "    d = []\n",
    "    for idx, col in enumerate(df.columns[2:]):\n",
    "        d.append(\n",
    "            {\n",
    "                'Catagory': col,\n",
    "                'Result': results[0][idx] > 0.1,\n",
    "                'Percent':  round((results[0][idx])*100,2)\n",
    "            }\n",
    "        )\n",
    "\n",
    "    d = pd.DataFrame(d)      \n",
    "        \n",
    "    return fcomment,d,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as interface:\n",
    "    name = gr.inputs.Textbox(lines=2, placeholder='Enter Your Sentence', label = \"Input Sentence\")\n",
    "    greet_btn = gr.Button(\"Submit\")\n",
    "    output = [gr.Textbox(label=\"Did you mean?\"),gr.Dataframe(label=\"Toxicity Detection & Classification\",headers=['Catagory', 'Result', 'Percent']) , gr.Textbox(label=\"Detoxification\")]\n",
    "    greet_btn.click(fn=score_comment, inputs=name, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
